% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/impkmeans.R
\name{imp_kmeans}
\alias{imp_kmeans}
\title{Perform improvement of k-means clustering}
\usage{
imp_kmeans(datasets, k, convergence = 1e-6)
}
\arguments{
\item{k}{the number of clusters.}

\item{convergence}{the convergence tolerance allowed.}

\item{dataset}{numeric matrix of data, or an object that can be coerced to such a matrix(such as a numeric vector or a data frame with all numeric columns).}
}
\details{
The k-means algorithm has at least two major theoretic shortcomings:

First, it has been shown that the worst case running time of the algorithm is super-polynomial in the input size.
Second, the approximation found can be arbitrarily bad with respect to the objective function compared to the optimal clustering.
The k-means++ algorithm addresses the second of these obstacles by specifying a procedure to initialize the cluster centers before proceeding with the standard k-means optimization iterations.

In data mining, k-means++ is an algorithm for choosing the initial values (or "seeds") for the k-means clustering algorithm. It was proposed in 2007 by David Arthur and Sergei Vassilvitskii, as an approximation algorithm for the NP-hard k-means problem a way of avoiding the sometimes poor clusterings found by the standard k-means algorithm.
}
\value{
\item{begining_centroids}{The beginning center point.}

\item{final_centroids}{The final center point.}

\item{label}{The cluster to which each point is assigned.}

\item{loss}{Total sum of squared.}

\item{iter}{The number of iterations.}
}
\description{
The intuition behind this approach is that spreading out the k initial cluster centers is a good thing: the first cluster center is chosen uniformly at random from the data points that are being clustered, after which each subsequent cluster center is chosen from the remaining data points with probability proportional to its squared distance from the point's closest existing cluster center.
}
\examples{
## a 2-dimensional example
data(iris)
iris = iris[, -5] #remove label
(k_means = imp_kmeans(iris, 3))
plot(iris$Petal.Length, iris$Petal.Width, col = k_means$label)

## b 2-dimensional example
x = rbind(matrix(rnorm(100, sd = 0.3), ncol = 2),
           matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2))
colnames(x) = c("x", "y")
(cl = imp_kmeans(x, 2))
plot(x, col = cl$label)
}
\references{
D. Arthur, S. Vassilvitskii, "K-Means++: The advantages of careful seeding", Proc. Symp. Discrete Algorithms, pp. 1027-1035, 2007

Kmeans++ in wiki.
}
\seealso{
\code{kmeans}
}
